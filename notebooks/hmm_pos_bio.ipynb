{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8903cb22-md",
   "metadata": {},
   "source": [
    "# HMM with BIO Tagging and POS\n",
    "\n",
    "This notebook implements an alternative Hidden Markov Model that incorporates:\n",
    "1. BIO tagging\n",
    "2. Part-of-Speech (POS)\n",
    "\n",
    "For detecting negation and uncertainty markers in multilingual Spanish and Catalan medical texts\n",
    "\n",
    "- Load the preprocessed data with BIO tagging and POS\n",
    "- Train the enhanced HMM model\n",
    "- Evaluate performance on test data\n",
    "- Compare with the baseline model\n",
    "- Save the model and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-setup",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.hmm import HMMBIOPOS\n",
    "from src.evaluation import *\n",
    "\n",
    "PATH_ROOT = os.path.dirname(os.getcwd()) \n",
    "\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"models\"), exist_ok=True) \n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\"), exist_ok=True)\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e37782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"Load preprocessed data from pickle file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "train_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_bio_pos.pkl\")\n",
    "test_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_bio_pos.pkl\")\n",
    "\n",
    "train_data = load_processed_data(train_file) \n",
    "test_data = load_processed_data(test_file)\n",
    "\n",
    "print(f\"Loaded {len(train_data['observations'])} training sequences\")\n",
    "print(f\"Loaded {len(test_data['observations'])} test sequences\")\n",
    "print(f\"Vocabulary size: {len(train_data['vocabulary'])}\")\n",
    "print(f\"State space (BIO format): {train_data['state_space']}\")\n",
    "\n",
    "tag_counts = {} # Check BIO tag distribution\n",
    "for state_seq in train_data[\"states\"]:\n",
    "    for state in state_seq:\n",
    "        if state not in tag_counts:\n",
    "            tag_counts[state] = 0\n",
    "        tag_counts[state] += 1\n",
    "\n",
    "print(\"\\nBIO Tag Distribution (Training):\")\n",
    "for tag, count in sorted(tag_counts.items()):\n",
    "    print(f\"  {tag}: {count} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "## Train HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-hmm",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_vocabulary = set()\n",
    "for obs_seq in train_data[\"observations\"]:\n",
    "    for obs in obs_seq:\n",
    "        if isinstance(obs, tuple) and len(obs) == 2:\n",
    "            bio_vocabulary.add(obs)  # Add word-POS tuple to vocabulary\n",
    "\n",
    "bio_pos_model = HMMBIOPOS( # Initialize and train the BIO+POS enhanced HMM model\n",
    "    state_space=train_data[\"state_space\"],\n",
    "    vocabulary=bio_vocabulary,\n",
    "    smoothing=0.01\n",
    ")\n",
    "\n",
    "print(\"Training BIO+POS enhanced HMM model...\")\n",
    "bio_pos_model.train(train_data[\"observations\"], train_data[\"states\"])  # Train the model\n",
    "\n",
    "model_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"models\", \"hmm_bio_pos.pkl\")  # Path for saving model\n",
    "bio_pos_model.save(model_path)  # Save the trained model\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-model",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = bio_pos_model.predict(test_data[\"observations\"])  # Generate predictions\n",
    "\n",
    "print(\"Evaluating predictions...\")\n",
    "metrics = compute_metrics(test_data[\"states\"], test_predictions, is_bio=True)  # Calculate token-level metrics\n",
    "entity_metrics = get_entity_based_metrics(test_data[\"states\"], test_predictions, is_bio=True)  # Calculate entity metrics\n",
    "scope_metrics = evaluate_scope_detection(test_data[\"states\"], test_predictions, test_data[\"observations\"], is_bio=True)  # Evaluate scope detection\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report (Token Level with BIO):\")\n",
    "print_classification_report(test_data[\"states\"], test_predictions, is_bio=True)\n",
    "\n",
    "print(\"\\nEntity-Level F1 Scores:\")\n",
    "for label, metrics_dict in entity_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}, Support = {metrics_dict.get('support', 'N/A')}\")\n",
    "print(f\"Macro Average: F1 = {entity_metrics['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nScope Detection F1 Scores:\")\n",
    "for label, metrics_dict in scope_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}\")\n",
    "print(f\"Macro Average: F1 = {scope_metrics['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "eval_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_bio_pos.json\")  # Path for saving evaluation\n",
    "save_metrics({\n",
    "    \"token_metrics\": metrics,\n",
    "    \"entity_metrics\": entity_metrics,\n",
    "    \"scope_metrics\": scope_metrics\n",
    "}, eval_path)  # Save metrics to file\n",
    "print(f\"Evaluation results saved to {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-baseline",
   "metadata": {},
   "source": [
    "## Compare with Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-baseline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_baseline.json\"), 'r') as f:\n",
    "        baseline_metrics = json.load(f) # Load baseline metrics\n",
    "        \n",
    "    baseline_token_f1 = baseline_metrics[\"token_metrics\"][\"macro_avg\"][\"f1\"]  # Get baseline token F1\n",
    "    baseline_entity_f1 = baseline_metrics[\"entity_metrics\"][\"macro_avg\"][\"f1\"]  # Get baseline entity F1\n",
    "    \n",
    "    bio_pos_token_f1 = metrics[\"macro_avg\"][\"f1\"]  # Get BIO+POS token F1\n",
    "    bio_pos_entity_f1 = entity_metrics[\"macro_avg\"][\"f1\"]  # Get BIO+POS entity F1\n",
    "    \n",
    "    labels = ['Token-Level F1', 'Entity-Level F1']  # Chart labels\n",
    "    baseline_scores = [baseline_token_f1, baseline_entity_f1]  # Baseline scores\n",
    "    bio_pos_scores = [bio_pos_token_f1, bio_pos_entity_f1]  # BIO+POS scores\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35 \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(x - width/2, baseline_scores, width, label='Baseline HMM')  # Plot baseline\n",
    "    ax.bar(x + width/2, bio_pos_scores, width, label='BIO+POS HMM')  # Plot BIO+POS\n",
    "    \n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('Performance Comparison: Baseline vs. BIO+POS Enhanced HMM')  # Chart title\n",
    "    ax.set_xticks(x) \n",
    "    ax.set_xticklabels(labels) \n",
    "    ax.legend() \n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, v in enumerate(baseline_scores):\n",
    "        ax.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    for i, v in enumerate(bio_pos_scores):\n",
    "        ax.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"bio_pos_vs_baseline.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    token_improvement = (bio_pos_token_f1 - baseline_token_f1) / baseline_token_f1 * 100  # Calculate token improvement\n",
    "    entity_improvement = (bio_pos_entity_f1 - baseline_entity_f1) / baseline_entity_f1 * 100  # Calculate entity improvement\n",
    "    \n",
    "    print(f\"Token-level F1 improvement: {token_improvement:.2f}%\") \n",
    "    print(f\"Entity-level F1 improvement: {entity_improvement:.2f}%\") \n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Baseline evaluation results not found. Run hmm_baseline.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyze-bio",
   "metadata": {},
   "source": [
    "## Analyze BIO Tagging Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-bio-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bio_tagging(test_states, test_predictions):\n",
    "    # Convert to standard (non-BIO) tags for comparison\n",
    "    std_test_states = [convert_bio_to_standard(seq) for seq in test_states]  # Convert true states\n",
    "    std_test_predictions = [convert_bio_to_standard(seq) for seq in test_predictions]  # Convert predicted states\n",
    "    \n",
    "    # Evaluate with and without BIO tagging\n",
    "    bio_metrics = get_entity_based_metrics(test_states, test_predictions, is_bio=True)  # Metrics with BIO\n",
    "    std_metrics = get_entity_based_metrics(std_test_states, std_test_predictions, is_bio=False)  # Metrics without BIO\n",
    "    \n",
    "    print(\"Entity-Level F1 Scores - Comparing BIO vs Standard Evaluation:\")\n",
    "    print(\"Entity Type\\tBIO Evaluation\\tStandard Evaluation\\tDifference\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for label in [\"NEG\", \"NSCO\", \"UNC\", \"USCO\"]:\n",
    "        bio_f1 = bio_metrics[label][\"f1\"]  # BIO F1 for label\n",
    "        std_f1 = std_metrics[label][\"f1\"]  # Standard F1 for label\n",
    "        diff = bio_f1 - std_f1  # Calculate difference\n",
    "        \n",
    "        print(f\"{label}\\t\\t{bio_f1:.4f}\\t\\t{std_f1:.4f}\\t\\t{diff:+.4f}\")\n",
    "\n",
    "    bio_macro_f1 = bio_metrics[\"macro_avg\"][\"f1\"]  # BIO macro F1\n",
    "    std_macro_f1 = std_metrics[\"macro_avg\"][\"f1\"]  # Standard macro F1\n",
    "    macro_diff = bio_macro_f1 - std_macro_f1  # Calculate macro difference\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Macro Avg\\t{bio_macro_f1:.4f}\\t\\t{std_macro_f1:.4f}\\t\\t{macro_diff:+.4f}\")\n",
    "    \n",
    "    return bio_metrics, std_metrics\n",
    "\n",
    "bio_metrics, std_metrics = analyze_bio_tagging(test_data[\"states\"], test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(test_data[\"state_space\"])  # Ensure labels are sorted\n",
    "flat_true_states = [tag for seq in test_data[\"states\"] for tag in seq]\n",
    "flat_pred_states = [tag for seq in test_predictions for tag in seq]\n",
    "\n",
    "report = classification_report(flat_true_states, flat_pred_states, labels=labels, target_names=labels, digits=4)\n",
    "\n",
    "print(\"Classification Report (Text Matrix):\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
