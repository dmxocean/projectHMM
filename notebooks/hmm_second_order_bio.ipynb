{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457b12ce-md",
   "metadata": {},
   "source": [
    "# Second-Order HMM\n",
    "\n",
    "This notebook implements a second-order (trigram) HMM that incorporates BIO tagging and POS information for detecting negation and uncertainty markers in multilingual Spanish and Catalan medical texts\n",
    "\n",
    "- Load preprocessed data with BIO tagging and POS\n",
    "- Train a second-order HMM model that captures entity boundaries and longer dependencies\n",
    "- Evaluate performance on test data\n",
    "- Compare results with the first-order BIO+POS model\n",
    "- Save the model and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-setup",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "PATH_ROOT = os.path.dirname(os.getcwd()) \n",
    "sys.path.append(\"..\") \n",
    "\n",
    "from src.hmm import HMMSecondOrder\n",
    "from src.evaluation import *\n",
    "\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\"), exist_ok=True)\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"Load preprocessed data from pickle file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "train_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_bio_pos.pkl\")  \n",
    "test_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_bio_pos.pkl\")    \n",
    "\n",
    "train_data = load_processed_data(train_file)  \n",
    "test_data = load_processed_data(test_file)    \n",
    "\n",
    "print(f\"Loaded {len(train_data['observations'])} training sequences\")\n",
    "print(f\"Loaded {len(test_data['observations'])} test sequences\")\n",
    "print(f\"Vocabulary size: {len(train_data['vocabulary'])}\")\n",
    "print(f\"State space (BIO format): {train_data['state_space']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "## Train Second-Order HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_vocabulary = set()\n",
    "for obs_seq in train_data[\"observations\"]:\n",
    "    for obs in obs_seq:\n",
    "        if isinstance(obs, tuple) and len(obs) == 2:\n",
    "            bio_vocabulary.add(obs)  # Add word-POS tuple to vocabulary\n",
    "\n",
    "second_order_model = HMMSecondOrder(\n",
    "    state_space=train_data[\"state_space\"],\n",
    "    vocabulary=bio_vocabulary,\n",
    "    smoothing=0.01\n",
    ")\n",
    "\n",
    "print(\"Training second-order BIO+POS HMM model...\")\n",
    "second_order_model.train(train_data[\"observations\"], train_data[\"states\"])  # Train the model\n",
    "\n",
    "model_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"models\", \"hmm_second_order.pkl\")\n",
    "second_order_model.save(model_path)  \n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-model",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = second_order_model.predict(test_data[\"observations\"])  # Generate predictions\n",
    "\n",
    "print(\"Evaluating predictions...\")\n",
    "metrics = compute_metrics(test_data[\"states\"], test_predictions, is_bio=True)  # Calculate token-level metrics\n",
    "entity_metrics = get_entity_based_metrics(test_data[\"states\"], test_predictions, is_bio=True)  # Entity metrics\n",
    "scope_metrics = evaluate_scope_detection(test_data[\"states\"], test_predictions, test_data[\"observations\"], is_bio=True)  # Evaluate scope detection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report (Token Level with BIO):\")\n",
    "print_classification_report(test_data[\"states\"], test_predictions, is_bio=True)\n",
    "\n",
    "print(\"\\nEntity-Level F1 Scores:\")\n",
    "for label, metrics_dict in entity_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}, Support = {metrics_dict.get('support', 'N/A')}\")\n",
    "print(f\"Macro Average: F1 = {entity_metrics['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nScope Detection F1 Scores:\")\n",
    "for label, metrics_dict in scope_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}\")\n",
    "print(f\"Macro Average: F1 = {scope_metrics['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "eval_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_second_order.json\")\n",
    "save_metrics({\n",
    "    \"token_metrics\": metrics,\n",
    "    \"entity_metrics\": entity_metrics,\n",
    "    \"scope_metrics\": scope_metrics\n",
    "}, eval_path)  # Save metrics to file\n",
    "print(f\"Evaluation results saved to {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-models",
   "metadata": {},
   "source": [
    "## Compare with First-Order BIO+POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_bio_pos.json\"), 'r') as f:\n",
    "        bio_pos_metrics = json.load(f)  # Load first-order evaluation results\n",
    "    \n",
    "    # Extract macro F1 scores\n",
    "    bio_pos_token_f1 = bio_pos_metrics[\"token_metrics\"][\"macro_avg\"][\"f1\"]  \n",
    "    bio_pos_entity_f1 = bio_pos_metrics[\"entity_metrics\"][\"macro_avg\"][\"f1\"]  \n",
    "    bio_pos_scope_f1 = bio_pos_metrics[\"scope_metrics\"][\"macro_avg\"][\"f1\"]  \n",
    "    \n",
    "    second_order_token_f1 = metrics[\"macro_avg\"][\"f1\"]  \n",
    "    second_order_entity_f1 = entity_metrics[\"macro_avg\"][\"f1\"]  \n",
    "    second_order_scope_f1 = scope_metrics[\"macro_avg\"][\"f1\"]  \n",
    "    \n",
    "    # Plot comparison\n",
    "    labels = [\"Token-Level F1\", \"Entity-Level F1\", \"Scope-Level F1\"]  \n",
    "    bio_pos_scores = [bio_pos_token_f1, bio_pos_entity_f1, bio_pos_scope_f1]  \n",
    "    second_order_scores = [second_order_token_f1, second_order_entity_f1, second_order_scope_f1]  \n",
    "    \n",
    "    x = np.arange(len(labels))  \n",
    "    width = 0.35 \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6)) \n",
    "    ax.bar(x - width/2, bio_pos_scores, width, label='First-Order BIO+POS HMM') \n",
    "    ax.bar(x + width/2, second_order_scores, width, label='Second-Order BIO+POS HMM') \n",
    "    \n",
    "    ax.set_ylabel('F1 Score') \n",
    "    ax.set_title('Performance Comparison: First-Order vs. Second-Order BIO+POS HMM') \n",
    "    ax.set_xticks(x) \n",
    "    ax.set_xticklabels(labels) \n",
    "    ax.legend() \n",
    "    \n",
    "   \n",
    "    for i, v in enumerate(bio_pos_scores):\n",
    "        ax.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    for i, v in enumerate(second_order_scores):\n",
    "        ax.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(os.path.join(PATH_ROOT, 'data', 'results', 'evaluation', 'first_vs_second_order.png')) \n",
    "    plt.show() \n",
    "    \n",
    "   \n",
    "    second_vs_first_token = (second_order_token_f1 - bio_pos_token_f1) / bio_pos_token_f1 * 100 \n",
    "    second_vs_first_entity = (second_order_entity_f1 - bio_pos_entity_f1) / bio_pos_entity_f1 * 100 \n",
    "    second_vs_first_scope = (second_order_scope_f1 - bio_pos_scope_f1) / bio_pos_scope_f1 * 100 \n",
    "    \n",
    "    print(f\"Second-Order vs First-Order (Token-level F1): {second_vs_first_token:.2f}% improvement\")\n",
    "    print(f\"Second-Order vs First-Order (Entity-level F1): {second_vs_first_entity:.2f}% improvement\")\n",
    "    print(f\"Second-Order vs First-Order (Scope-level F1): {second_vs_first_scope:.2f}% improvement\")\n",
    "    \n",
    "    print(\"\\nEntity-level F1 comparison by entity type:\")\n",
    "    print(\"Entity Type\\tFirst-Order\\tSecond-Order\\tImprovement\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for entity_type in [\"NEG\", \"NSCO\", \"UNC\", \"USCO\"]:\n",
    "        first_f1 = bio_pos_metrics[\"entity_metrics\"][entity_type][\"f1\"]  # First-order F1 for entity type\n",
    "        second_f1 = entity_metrics[entity_type][\"f1\"]  # Second-order F1 for entity type\n",
    "        improvement = (second_f1 - first_f1) / first_f1 * 100  # Calculate improvement percentage\n",
    "        \n",
    "        print(f\"{entity_type}\\t\\t{first_f1:.4f}\\t\\t{second_f1:.4f}\\t\\t{improvement:+.2f}%\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"First-order BIO+POS evaluation results not found. Run hmm_BIO_POS.ipynb first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
