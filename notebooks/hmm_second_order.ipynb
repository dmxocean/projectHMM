{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-markdown",
   "metadata": {},
   "source": [
    "# Second-Order HMM\n",
    "\n",
    "This notebook implements a second-order (trigram) HMM for detecting negation and uncertainty markers in multilingual Spanish and Catalan medical texts\n",
    "\n",
    "- Load preprocessed baseline data (without BIO tagging or POS)\n",
    "- Train a second-order HMM model that captures longer dependencies\n",
    "- Evaluate performance on test data\n",
    "- Compare results with the first-order baseline model\n",
    "- Save the model and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-setup",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "PATH_ROOT = os.path.dirname(os.getcwd()) \n",
    "sys.path.append(\"..\") \n",
    "\n",
    "from src.hmm import HMMSecondOrderBaseline\n",
    "from src.evaluation import *\n",
    "\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\"), exist_ok=True)\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"Load preprocessed data from pickle file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "train_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_baseline.pkl\")  \n",
    "test_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_baseline.pkl\")    \n",
    "\n",
    "train_data = load_processed_data(train_file)\n",
    "test_data = load_processed_data(test_file)    \n",
    "\n",
    "print(f\"Loaded {len(train_data['observations'])} training sequences\")\n",
    "print(f\"Loaded {len(test_data['observations'])} test sequences\")\n",
    "print(f\"Vocabulary size: {len(train_data['vocabulary'])}\")\n",
    "print(f\"State space: {train_data['state_space']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "## Train Second-Order HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_model = HMMSecondOrderBaseline(\n",
    "    state_space=train_data[\"state_space\"],\n",
    "    vocabulary=train_data[\"vocabulary\"],\n",
    "    smoothing=0.01\n",
    ")\n",
    "\n",
    "print(\"Training second-order HMM model...\")\n",
    "second_order_model.train(train_data[\"observations\"], train_data[\"states\"])\n",
    "\n",
    "model_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"models\", \"hmm_second_order.pkl\")\n",
    "second_order_model.save(model_path)  \n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-model",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = second_order_model.predict(test_data[\"observations\"])\n",
    "\n",
    "print(\"Evaluating predictions...\")\n",
    "metrics = compute_metrics(test_data[\"states\"], test_predictions, is_bio=False)\n",
    "entity_metrics = get_entity_based_metrics(test_data[\"states\"], test_predictions, is_bio=False)\n",
    "scope_metrics = evaluate_scope_detection(test_data[\"states\"], test_predictions, test_data[\"observations\"], is_bio=False)\n",
    "\n",
    "print(\"\\nClassification Report (Token Level):\")\n",
    "print_classification_report(test_data[\"states\"], test_predictions, is_bio=False)\n",
    "\n",
    "print(\"\\nEntity-Level F1 Scores:\")\n",
    "for label, metrics_dict in entity_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}, Support = {metrics_dict.get('support', 'N/A')}\")\n",
    "print(f\"Macro Average: F1 = {entity_metrics['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nScope Detection F1 Scores:\")\n",
    "for label, metrics_dict in scope_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}\")\n",
    "print(f\"Macro Average: F1 = {scope_metrics['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "eval_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_second_order.json\")\n",
    "save_metrics({\n",
    "    \"token_metrics\": metrics,\n",
    "    \"entity_metrics\": entity_metrics,\n",
    "    \"scope_metrics\": scope_metrics\n",
    "}, eval_path)  \n",
    "print(f\"Evaluation results saved to {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-models",
   "metadata": {},
   "source": [
    "## Compare with Baseline HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved code for comparing baseline and second-order models\n",
    "\n",
    "try:\n",
    "    # Load baseline evaluation results\n",
    "    baseline_json_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_baseline.json\")\n",
    "    with open(baseline_json_path, 'r') as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    \n",
    "    # First check the structure of the baseline metrics file\n",
    "    print(f\"Structure of baseline metrics file: {list(baseline_metrics.keys())}\")\n",
    "    \n",
    "    # Determine the structure and extract metrics accordingly\n",
    "    if \"token_metrics\" in baseline_metrics:\n",
    "        # Nested structure with top-level keys\n",
    "        baseline_token_f1 = baseline_metrics[\"token_metrics\"][\"macro_avg\"][\"f1\"]\n",
    "        baseline_entity_f1 = baseline_metrics[\"entity_metrics\"][\"macro_avg\"][\"f1\"]\n",
    "        has_scope = \"scope_metrics\" in baseline_metrics\n",
    "        if has_scope:\n",
    "            baseline_scope_f1 = baseline_metrics[\"scope_metrics\"][\"macro_avg\"][\"f1\"]\n",
    "    else:\n",
    "        # Flat structure (metrics directly at top level)\n",
    "        baseline_token_f1 = baseline_metrics[\"macro_avg\"][\"f1\"]\n",
    "        baseline_entity_f1 = entity_metrics[\"macro_avg\"][\"f1\"]\n",
    "        has_scope = False\n",
    "    \n",
    "    # Current second-order metrics\n",
    "    second_order_token_f1 = metrics[\"macro_avg\"][\"f1\"]\n",
    "    second_order_entity_f1 = entity_metrics[\"macro_avg\"][\"f1\"]\n",
    "    second_order_scope_f1 = scope_metrics[\"macro_avg\"][\"f1\"]\n",
    "    \n",
    "    # Prepare comparison data based on available metrics\n",
    "    if has_scope:\n",
    "        # If scope metrics are available in both\n",
    "        labels = [\"Token-Level F1\", \"Entity-Level F1\", \"Scope-Level F1\"]\n",
    "        baseline_scores = [baseline_token_f1, baseline_entity_f1, baseline_scope_f1]\n",
    "        second_order_scores = [second_order_token_f1, second_order_entity_f1, second_order_scope_f1]\n",
    "    else:\n",
    "        # If scope metrics are only in second-order\n",
    "        print(\"Note: Scope metrics not found in baseline evaluation. Only comparing token and entity metrics.\")\n",
    "        labels = [\"Token-Level F1\", \"Entity-Level F1\"]\n",
    "        baseline_scores = [baseline_token_f1, baseline_entity_f1]\n",
    "        second_order_scores = [second_order_token_f1, second_order_entity_f1]\n",
    "    \n",
    "    # Create visualization\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, baseline_scores, width, label='First-Order Baseline HMM')\n",
    "    ax.bar(x + width/2, second_order_scores[:len(baseline_scores)], width, label='Second-Order Baseline HMM')\n",
    "    \n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('Performance Comparison: First-Order vs. Second-Order Baseline HMM')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels to bars\n",
    "    for i, v in enumerate(baseline_scores):\n",
    "        ax.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    for i, v in enumerate(second_order_scores[:len(baseline_scores)]):\n",
    "        ax.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PATH_ROOT, 'data', 'results', 'evaluation', 'second_order_baseline_vs_baseline.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print performance improvements\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Token-level improvement\n",
    "    second_vs_baseline_token = (second_order_token_f1 - baseline_token_f1) / baseline_token_f1 * 100\n",
    "    print(f\"Second-Order vs Baseline (Token-level F1): {second_vs_baseline_token:.2f}% improvement\")\n",
    "    \n",
    "    # Entity-level improvement\n",
    "    second_vs_baseline_entity = (second_order_entity_f1 - baseline_entity_f1) / baseline_entity_f1 * 100\n",
    "    print(f\"Second-Order vs Baseline (Entity-level F1): {second_vs_baseline_entity:.2f}% improvement\")\n",
    "    \n",
    "    # Scope-level improvement (if available)\n",
    "    if has_scope:\n",
    "        second_vs_baseline_scope = (second_order_scope_f1 - baseline_scope_f1) / baseline_scope_f1 * 100\n",
    "        print(f\"Second-Order vs Baseline (Scope-level F1): {second_vs_baseline_scope:.2f}% improvement\")\n",
    "    \n",
    "    # Entity-type level comparison\n",
    "    print(\"\\nEntity-level F1 comparison by entity type:\")\n",
    "    print(\"Entity Type\\tBaseline\\tSecond-Order\\tImprovement\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for entity_type in [\"NEG\", \"NSCO\", \"UNC\", \"USCO\"]:\n",
    "        # Get baseline F1 score based on structure\n",
    "        if \"entity_metrics\" in baseline_metrics:\n",
    "            baseline_f1 = baseline_metrics[\"entity_metrics\"][entity_type][\"f1\"]\n",
    "        else:\n",
    "            # Try direct access if flat structure\n",
    "            try:\n",
    "                baseline_f1 = baseline_metrics[entity_type][\"f1\"]\n",
    "            except KeyError:\n",
    "                print(f\"Could not find {entity_type} metrics in baseline evaluation\")\n",
    "                continue\n",
    "            \n",
    "        second_order_f1 = entity_metrics[entity_type][\"f1\"]\n",
    "        improvement = (second_order_f1 - baseline_f1) / baseline_f1 * 100\n",
    "        \n",
    "        print(f\"{entity_type}\\t\\t{baseline_f1:.4f}\\t\\t{second_order_f1:.4f}\\t\\t{improvement:+.2f}%\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Baseline evaluation results not found at {baseline_json_path}.\")\n",
    "    print(\"Run hmm_baseline.ipynb first and ensure it saves evaluation results.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Key {e} not found in metrics file.\")\n",
    "    print(\"Structure of baseline_metrics:\", list(baseline_metrics.keys()) if 'baseline_metrics' in locals() else \"Unknown\")\n",
    "    if 'baseline_metrics' in locals() and \"token_metrics\" in baseline_metrics:\n",
    "        print(\"Available keys in token_metrics:\", list(baseline_metrics[\"token_metrics\"].keys()))\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during comparison: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
