{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c700690-md",
   "metadata": {},
   "source": [
    "# HMM Baseline\n",
    "\n",
    "This notebook implements a baseline Hidden Markov Model for detecting negation and uncertainty markers in multilingual Spanish and Catalan medical texts\n",
    "\n",
    "- Load the preprocessed data\n",
    "- Train a basic HMM model\n",
    "- Evaluate performance on test data\n",
    "- Save the model and results\n",
    "\n",
    "The baseline model uses only word tokens as observations (without POS tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-setup",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c700690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.hmm import HMMBaseline\n",
    "from src.evaluation import *\n",
    "\n",
    "PATH_ROOT = os.path.dirname(os.getcwd()) \n",
    "\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"models\"), exist_ok=True) \n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\"), exist_ok=True)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"Load preprocessed data from pickle file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# Load the preprocessed data\n",
    "train_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_baseline.pkl\")\n",
    "test_file = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_baseline.pkl\")\n",
    "\n",
    "train_data = load_processed_data(train_file)  \n",
    "test_data = load_processed_data(test_file)  \n",
    "\n",
    "print(f\"Loaded {len(train_data['observations'])} training sequences\")\n",
    "print(f\"Loaded {len(test_data['observations'])} test sequences\")\n",
    "print(f\"Vocabulary size: {len(train_data['vocabulary'])}\")\n",
    "print(f\"State space: {train_data['state_space']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-model",
   "metadata": {},
   "source": [
    "## Train the Baseline HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = HMMBaseline(\n",
    "    state_space=train_data[\"state_space\"],\n",
    "    vocabulary=train_data[\"vocabulary\"],\n",
    "    smoothing=0.01\n",
    ")\n",
    "\n",
    "print(\"Training HMM model...\")\n",
    "hmm_model.train(train_data[\"observations\"], train_data[\"states\"])  # Train using observations and states\n",
    "\n",
    "model_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"models\", \"hmm_baseline.pkl\")  # Path for saving model\n",
    "hmm_model.save(model_path)  # Save the trained model\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-model",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions on test data...\")\n",
    "test_predictions = hmm_model.predict(test_data[\"observations\"])  # Predict on test data\n",
    "\n",
    "print(\"Evaluating predictions...\") # Using evaluation functions \n",
    "metrics = compute_metrics(test_data[\"states\"], test_predictions)  # Calculate token-level metrics\n",
    "entity_metrics = get_entity_based_metrics(test_data[\"states\"], test_predictions)  # Calculate entity-level metrics\n",
    "\n",
    "print(\"\\nClassification Report (Token Level):\")\n",
    "print_classification_report(test_data[\"states\"], test_predictions)\n",
    "\n",
    "print(\"\\nEntity-Level F1 Scores:\")\n",
    "for label, metrics_dict in entity_metrics.items():\n",
    "    if label != \"macro_avg\":\n",
    "        print(f\"{label}: F1 = {metrics_dict['f1']:.4f}, Precision = {metrics_dict['precision']:.4f}, \"\n",
    "              f\"Recall = {metrics_dict['recall']:.4f}, Support = {metrics_dict.get('support', 'N/A')}\")\n",
    "print(f\"Macro Average: F1 = {entity_metrics['macro_avg']['f1']:.4f}\\n\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_path = os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_baseline.json\")  # Path for saving evaluation\n",
    "save_metrics({\n",
    "    \"token_metrics\": metrics,\n",
    "    \"entity_metrics\": entity_metrics\n",
    "}, eval_path)  # Save metrics to file\n",
    "\n",
    "print(f\"Evaluation results saved to {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "language-analysis",
   "metadata": {},
   "source": [
    "## Language-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "language-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_language_info(data):\n",
    "    token_languages = []\n",
    "\n",
    "    for sequence in data[\"sequences\"]:\n",
    "        seq_langs = []\n",
    "        for token in sequence[\"tokens\"]:\n",
    "            seq_langs.append(token.get(\"language\", \"es\"))  # Default to Spanish if not specified\n",
    "        token_languages.append(seq_langs)\n",
    "    \n",
    "    return token_languages\n",
    "\n",
    "token_languages = extract_language_info(test_data)  # Get language information\n",
    "\n",
    "language_metrics = analyze_by_language(\n",
    "    test_data[\"states\"],\n",
    "    test_predictions,\n",
    "    token_languages\n",
    ")\n",
    "\n",
    "print(\"\\nLanguage-Specific Performance:\")\n",
    "print(f\"Spanish Token-level F1: {language_metrics['spanish']['macro_avg']['f1']:.4f}\")\n",
    "print(f\"Catalan Token-level F1: {language_metrics['catalan']['macro_avg']['f1']:.4f}\")\n",
    "\n",
    "eval_path_lang = os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"evaluation_baseline_language.json\")  # Path for language metrics\n",
    "save_metrics(language_metrics, eval_path_lang)  # Save language-specific evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-transitions",
   "metadata": {},
   "source": [
    "## Visualize Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d66e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model.transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-transitions-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(hmm_model.transition_probs, cmap=\"viridis\")  # Plot transition matrix\n",
    "plt.colorbar()\n",
    "plt.title(\"Transition Probabilities\")\n",
    "plt.xlabel(\"To State\")  \n",
    "plt.ylabel(\"From State\")\n",
    "plt.xticks(range(len(hmm_model.state_space)), hmm_model.state_space, rotation=45)\n",
    "plt.yticks(range(len(hmm_model.state_space)), hmm_model.state_space)  \n",
    "plt.tight_layout() \n",
    "plt.savefig(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"baseline_transitions.png\"))\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-emissions",
   "metadata": {},
   "source": [
    "## Visualize Top Emission Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-emissions-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(state_idx, n=10):\n",
    "    \"\"\"Function to find top words for each state\"\"\"\n",
    "    probs = hmm_model.emission_probs[state_idx, :]  # Probabilities for  state\n",
    "    top_indices = np.argsort(probs)[-n:][::-1]  # Descending order indices\n",
    "    top_words = [hmm_model.vocabulary[i] for i in top_indices]  # Get words for indices\n",
    "    top_probs = [probs[i] for i in top_indices]  # Get corresponding probabilities\n",
    "    return top_words, top_probs\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "states_to_plot = [\"NEG\", \"NSCO\", \"UNC\", \"USCO\"]\n",
    "for i, state in enumerate(states_to_plot):\n",
    "    plt.subplot(2, 2, i+1) \n",
    "    if state in hmm_model.state_to_idx:\n",
    "        state_idx = hmm_model.state_to_idx[state]  \n",
    "        words, probs = get_top_words(state_idx, 10)\n",
    "        \n",
    "        plt.bar(range(len(words)), probs)  \n",
    "        plt.xticks(range(len(words)), words, rotation=45, ha=\"right\")  \n",
    "        plt.title(f\"Top words for {state}\")  \n",
    "        plt.tight_layout()  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PATH_ROOT, \"data\", \"results\", \"evaluation\", \"baseline_top_words.png\")) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
