{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "This notebook performs an analysis of multilingual medical text text cleaning rules:\n",
    "- Document structure analysis\n",
    "- Duplicate document detection\n",
    "- Annotation and label distribution\n",
    "- Sentence-level language detection\n",
    "- Text cleaning pattern analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Set, Any, Optional\n",
    "import spacy\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "PATH_ROOT = os.path.dirname(os.getcwd())  # Path to the root of the project\n",
    "\n",
    "# Create directories for output\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"processed\"), exist_ok=True)\n",
    "\n",
    "try:  # Load spaCy models\n",
    "    nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "    nlp_ca = spacy.load(\"ca_core_news_sm\")\n",
    "    print(\"spaCy models loaded successfully\\n\")\n",
    "except OSError:\n",
    "    print(\"ERROR: You need to download spaCy models\")\n",
    "    print(\"python -m spacy download es_core_news_sm\")\n",
    "    print(\"python -m spacy download ca_core_news_sm\")\n",
    "\n",
    "# Add project root to system path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "try:  # Import utility functions\n",
    "    from utils.json import *\n",
    "    from utils.language import *\n",
    "except ImportError:\n",
    "    print(\"Warning: Unable to import utility modules. Some functions may not work\")\n",
    "\n",
    "# File paths\n",
    "FILE_TRAIN = os.path.join(PATH_ROOT, \"data\", \"raw\", \"train.json\")\n",
    "FILE_TEST = os.path.join(PATH_ROOT, \"data\", \"raw\", \"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Try loading train data\n",
    "for file_path in [FILE_TRAIN, FILE_TEST]:\n",
    "    if not data:\n",
    "        data = load_json_data(file_path)\n",
    "        if data:\n",
    "            print(f\"Loaded {len(data)} documents\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DOCUMENT STRUCTURE ANALYSIS\\n\")\n",
    "structure = explore_json_structure(data)  # Explore document structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DUPLICATE DOCUMENT ANALYSIS\\n\")\n",
    "\n",
    "if not data:\n",
    "    print(\"No data available for duplicate analysis\")\n",
    "\n",
    "true_counts = {}\n",
    "\n",
    "# Count each unique document ID\n",
    "for doc in data:\n",
    "    if \"data\" in doc and \"id\" in doc[\"data\"]:\n",
    "        doc_id = doc[\"data\"][\"id\"]\n",
    "        true_counts[doc_id] = true_counts.get(doc_id, 0) + 1\n",
    "\n",
    "# Find actually duplicated IDs\n",
    "actual_duplicates = {id: count for id, count in true_counts.items() if count > 1}\n",
    "\n",
    "print(f\"Number of documents: {len(data)}\")\n",
    "print(f\"Number of unique document IDs: {len(true_counts)}\")\n",
    "print(f\"Number of truly duplicated IDs: {len(actual_duplicates)}\")\n",
    "print()\n",
    "\n",
    "if actual_duplicates:\n",
    "    print(\"True duplicate document IDs:\")\n",
    "    for doc_id, count in sorted(actual_duplicates.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"ID {doc_id} appears {count} times\")\n",
    "else:\n",
    "    print(\"No duplicate document IDs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract annotations from documents\n",
    "    \"\"\"\n",
    "\n",
    "    all_annotations = []\n",
    "\n",
    "    for doc in data:\n",
    "        if \"data\" not in doc or \"id\" not in doc[\"data\"] or \"text\" not in doc[\"data\"]:\n",
    "            continue\n",
    "\n",
    "        doc_id = doc[\"data\"][\"id\"]\n",
    "        text = doc[\"data\"][\"text\"]\n",
    "\n",
    "        # Extract annotations\n",
    "        if \"predictions\" in doc and doc[\"predictions\"]:\n",
    "            for pred in doc[\"predictions\"]:\n",
    "                if \"result\" in pred and pred[\"result\"]:\n",
    "                    for result in pred[\"result\"]:\n",
    "                        if \"value\" in result and \"labels\" in result[\"value\"] and result[\"value\"][\"labels\"]:\n",
    "                            value = result[\"value\"]\n",
    "                            start = value.get(\"start\")\n",
    "                            end = value.get(\"end\")\n",
    "                            labels = value.get(\"labels\", [])\n",
    "\n",
    "                            if start is not None and end is not None and start < len(text) and end <= len(text):\n",
    "                                annotation_text = text[start:end]\n",
    "\n",
    "                                for label in labels:\n",
    "                                    all_annotations.append(\n",
    "                                        {\n",
    "                                            \"doc_id\": doc_id,\n",
    "                                            \"annotation_id\": result.get(\"id\"),\n",
    "                                            \"start\": start,\n",
    "                                            \"end\": end,\n",
    "                                            \"label\": label,\n",
    "                                            \"text\": annotation_text,\n",
    "                                            \"length\": len(annotation_text),\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if all_annotations:\n",
    "        return pd.DataFrame(all_annotations)\n",
    "    else:\n",
    "        # Return empty DataFrame with expected columns\n",
    "        return pd.DataFrame(columns=[\"doc_id\", \"annotation_id\", \"start\", \"end\", \"label\", \"text\", \"length\"])\n",
    "\n",
    "\n",
    "annotations_df = extract_annotations()  # Extract annotations\n",
    "\n",
    "\n",
    "print(\"LABEL DISTRIBUTION ANALYSIS\\n\")\n",
    "\n",
    "if annotations_df.empty:\n",
    "    print(\"No annotations found for analysis\")\n",
    "\n",
    "# Get label counts\n",
    "label_counts = annotations_df[\"label\"].value_counts()\n",
    "\n",
    "print(f\"Found {len(annotations_df)} annotations across {annotations_df.doc_id.nunique()} documents\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  - {label}: {count} annotations ({count/len(annotations_df)*100:.1f}%)\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=label_counts.index, y=label_counts.values, hue=label_counts.index, palette=\"viridis\", legend=False)\n",
    "plt.title(\"Distribution of Annotation Labels\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add count values on top of bars\n",
    "for i, count in enumerate(label_counts.values):\n",
    "    ax.text(i, count + 25, str(count), ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample annotations:\")\n",
    "sample_size = min(5, len(annotations_df))\n",
    "for i, (_, row) in enumerate(annotations_df.sample(sample_size).iterrows()):\n",
    "    print(f\"  {i+1}. Label: {row.label}\")\n",
    "    print(f\"\\tText: '{row.text}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Level Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(data, max_docs=100) -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Extract and process sentences from documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Extracting sentences from up to {max_docs} documents...\")\n",
    "    sentences_by_lang = {\"es\": [], \"ca\": []}\n",
    "    doc_languages = {\"mixed\": 0, \"es_only\": 0, \"ca_only\": 0, \"total\": 0}\n",
    "    processed = 0\n",
    "\n",
    "    for i, doc in enumerate(data):\n",
    "        if processed >= max_docs:\n",
    "            break\n",
    "\n",
    "        if \"data\" not in doc or \"text\" not in doc[\"data\"] or not doc[\"data\"][\"text\"]:\n",
    "            continue\n",
    "\n",
    "        text = doc[\"data\"][\"text\"]\n",
    "        doc_id = doc[\"data\"].get(\"id\", f\"doc_{i}\")\n",
    "        doc_langs = set()\n",
    "\n",
    "        try:  # Segment document into sentences\n",
    "            doc_nlp = nlp_es(text)\n",
    "            # Process each sentence\n",
    "            sent_found = False\n",
    "\n",
    "            for sent in doc_nlp.sents:  # Get sentences using spaCy\n",
    "                sent_text = sent.text.strip()\n",
    "                if not sent_text or len(sent_text) < 10:\n",
    "                    continue\n",
    "\n",
    "                sent_found = True\n",
    "                sent_lang = detect_sentence_language(sent_text)\n",
    "                doc_langs.add(sent_lang)\n",
    "                sentences_by_lang[sent_lang].append({\"text\": sent_text, \"doc_id\": doc_id})\n",
    "\n",
    "            # Fallback to simple sentence splitting if spaCy doesn't find sentences\n",
    "            if not sent_found:\n",
    "                simple_sentences = [s.strip() for s in re.split(r\"[.!?]\\s+\", text) if len(s.strip()) > 10]\n",
    "                for sent_text in simple_sentences:\n",
    "                    sent_lang = detect_sentence_language(sent_text)\n",
    "                    doc_langs.add(sent_lang)\n",
    "                    sentences_by_lang[sent_lang].append({\"text\": sent_text, \"doc_id\": doc_id})\n",
    "\n",
    "            # Update document language statistics\n",
    "            if len(doc_langs) > 1:\n",
    "                doc_languages[\"mixed\"] += 1\n",
    "            elif \"es\" in doc_langs:\n",
    "                doc_languages[\"es_only\"] += 1\n",
    "            elif \"ca\" in doc_langs:\n",
    "                doc_languages[\"ca_only\"] += 1\n",
    "\n",
    "            doc_languages[\"total\"] += 1\n",
    "            processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Print statistics\n",
    "    es_count = len(sentences_by_lang[\"es\"])\n",
    "    ca_count = len(sentences_by_lang[\"ca\"])\n",
    "    total = es_count + ca_count\n",
    "\n",
    "    print(f\"Extracted {total} sentences from {processed} documents\")\n",
    "    if total > 0:\n",
    "        print(f\"Spanish: {es_count} sentences ({es_count/total*100:.1f}%)\")\n",
    "        print(f\"Catalan: {ca_count} sentences ({ca_count/total*100:.1f}%)\")\n",
    "\n",
    "        # Document language composition\n",
    "        print(\"\\nDocument language composition:\")\n",
    "        if doc_languages[\"total\"] > 0:\n",
    "            print(f\"Spanish only: {doc_languages['es_only']} ({doc_languages['es_only']/doc_languages['total']*100:.1f}%)\")\n",
    "            print(f\"Catalan only: {doc_languages['ca_only']} ({doc_languages['ca_only']/doc_languages['total']*100:.1f}%)\")\n",
    "            print(f\"Mixed languages: {doc_languages['mixed']} ({doc_languages['mixed']/doc_languages['total']*100:.1f}%)\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))  # Visual distribution of sentences by language\n",
    "        plt.pie([es_count, ca_count], labels=[\"Spanish\", \"Catalan\"], autopct=\"%1.1f%%\", colors=[\"#3274A1\", \"#E1812C\"], startangle=90)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.title(\"Sentence Language Distribution\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return sentences_by_lang\n",
    "\n",
    "\n",
    "print(\"SENTENCE EXTRACTION AND LANGUAGE DETECTION\\n\")\n",
    "sentences = extract_sentences(data)  # Extract sentences\n",
    "\n",
    "# Show sample sentences\n",
    "print(\"\\nSample sentences:\")\n",
    "for lang, name in [(\"es\", \"Spanish\"), (\"ca\", \"Catalan\")]:\n",
    "    print(f\"\\n{name} examples:\")\n",
    "    for i, sent in enumerate(sentences[lang][:3]):\n",
    "        print(f\"  {i+1}. {sent['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Pattern Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abbreviation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ABBREVIATION ANALYSIS \")\n",
    "print(\"Finding abbreviations that should be expanded during text cleaning\\n\")\n",
    "\n",
    "for lang, name in [(\"es\", \"SPANISH\"), (\"ca\", \"CATALAN\")]:\n",
    "    # Find standard abbreviations ending with period\n",
    "    abbrev_pattern = r\"\\b([a-zA-ZáéíóúüñçÁÉÍÓÚÜÑÇàèìòù]{1,5}\\.)\\s\"\n",
    "    abbrevs_count = Counter()\n",
    "    abbrevs_context = {}\n",
    "\n",
    "    for sent_data in sentences[lang]:\n",
    "        text = sent_data[\"text\"]\n",
    "        for match in re.finditer(abbrev_pattern, text):\n",
    "            abbrev = match.group(1)\n",
    "            abbrevs_count[abbrev] += 1\n",
    "            # Store context if we don't have it yet\n",
    "            if abbrev not in abbrevs_context:\n",
    "                start = max(0, match.start() - 15)\n",
    "                end = min(len(text), match.end() + 15)\n",
    "                abbrevs_context[abbrev] = text[start:end]\n",
    "\n",
    "    print(f\"{name} ABBREVIATIONS:\")\n",
    "    if abbrevs_count:\n",
    "        for abbrev, count in abbrevs_count.most_common(10):\n",
    "            context = abbrevs_context.get(abbrev, \"\")\n",
    "            print(f\"  {abbrev} ({count} occurrences)\")\n",
    "            print(f\"    Context: '{context}'\")\n",
    "            # Suggest expansion\n",
    "            expansion = \"\"\n",
    "            if abbrev.lower() == \"dr.\":\n",
    "                expansion = \"doctor\" if lang == \"es\" else \"doctor\"\n",
    "            elif abbrev.lower() == \"dra.\":\n",
    "                expansion = \"doctora\" if lang == \"es\" else \"doctora\"\n",
    "            elif abbrev.lower() == \"d.\":\n",
    "                expansion = \"de\"\n",
    "            elif abbrev.lower() == \"sr.\":\n",
    "                expansion = \"señor\" if lang == \"es\" else \"senyor\"\n",
    "            elif abbrev.lower() == \"sra.\":\n",
    "                expansion = \"señora\" if lang == \"es\" else \"senyora\"\n",
    "            elif abbrev.lower() == \"t.a.\":\n",
    "                expansion = \"tensión arterial\" if lang == \"es\" else \"tensió arterial\"\n",
    "            elif abbrev.lower() == \"hosp.\":\n",
    "                expansion = \"hospital\"\n",
    "            elif abbrev.lower() == \"tto.\":\n",
    "                expansion = \"tratamiento\" if lang == \"es\" else \"tractament\"\n",
    "            else:\n",
    "                expansion = abbrev.lower().replace(\".\", \"\")\n",
    "            if expansion:\n",
    "                print(f\"    Suggested expansion: '{abbrev}' → '{expansion}'\\n\")\n",
    "    else:\n",
    "        print(\"  No abbreviations found.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language-Specific Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LANGUAGE-SPECIFIC PATTERN ANALYSIS \")\n",
    "print(\"Finding language-specific patterns like apostrophes and contractions\\n\")\n",
    "\n",
    "# Catalan apostrophes\n",
    "print(\"CATALAN APOSTROPHES:\")\n",
    "apostrophe_pattern = r\"\\b([ldmnst]\\'[a-zàèìòùáéíóúçñ]+)\"\n",
    "apostrophe_types = Counter()\n",
    "apostrophe_examples = defaultdict(list)\n",
    "\n",
    "for sent_data in sentences[\"ca\"]:\n",
    "    text = sent_data[\"text\"]\n",
    "    for match in re.finditer(apostrophe_pattern, text, re.IGNORECASE):\n",
    "        found = match.group(1).lower()\n",
    "        apos_type = found[0] + \"'\"\n",
    "        apostrophe_types[apos_type] += 1\n",
    "        if len(apostrophe_examples[apos_type]) < 3:  # Store a few examples\n",
    "            start = max(0, match.start() - 15)\n",
    "            end = min(len(text), match.end() + 15)\n",
    "            context = text[start:end]\n",
    "            apostrophe_examples[apos_type].append((found, context))\n",
    "\n",
    "if apostrophe_types:\n",
    "    for apos_type, count in apostrophe_types.most_common():\n",
    "        print(f\"  {apos_type} ({count} occurrences)\")\n",
    "        for example, context in apostrophe_examples[apos_type]:  # Show examples\n",
    "            print(f\"    Example: '{example}'\")\n",
    "            print(f\"    Context: '{context}'\\n\")\n",
    "        # Suggest replacement\n",
    "        if apos_type == \"d'\":\n",
    "            print(f\"    Current function transforms: '{apos_type}example' → 'de example'\")\n",
    "        elif apos_type == \"l'\":\n",
    "            print(f\"    Current function transforms: '{apos_type}example' → 'el example'\")\n",
    "        else:  # For other types not handled in current function\n",
    "            letter = apos_type[0]\n",
    "            if letter == \"n\":\n",
    "                replacement = \"en\"\n",
    "            elif letter == \"s\":\n",
    "                replacement = \"se\"\n",
    "            elif letter == \"m\":\n",
    "                replacement = \"me\"\n",
    "            elif letter == \"t\":\n",
    "                replacement = \"te\"\n",
    "            else:\n",
    "                replacement = letter + \"e\"\n",
    "            print(f\"    Suggested transformation: '{apos_type}example' → '{replacement} example'\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  No Catalan apostrophes found.\")\n",
    "    print()\n",
    "\n",
    "# Spanish contractions\n",
    "print(\"SPANISH CONTRACTIONS:\")\n",
    "contraction_pattern = r\"\\b(del|al|d el|a el)\\b\"\n",
    "contraction_count = Counter()\n",
    "contraction_examples = defaultdict(list)\n",
    "\n",
    "for sent_data in sentences[\"es\"]:\n",
    "    text = sent_data[\"text\"]\n",
    "    for match in re.finditer(contraction_pattern, text, re.IGNORECASE):\n",
    "        found = match.group(1).lower()\n",
    "        contraction_count[found] += 1\n",
    "        # Store a few examples\n",
    "        if len(contraction_examples[found]) < 3:\n",
    "            start = max(0, match.start() - 15)\n",
    "            end = min(len(text), match.end() + 15)\n",
    "            context = text[start:end]\n",
    "            contraction_examples[found].append(context)\n",
    "\n",
    "if contraction_count:\n",
    "    for contraction, count in contraction_count.most_common():\n",
    "        print(f\"  {contraction} ({count} occurrences)\")\n",
    "        # Show examples\n",
    "        for context in contraction_examples[contraction]:\n",
    "            print(f\"    Context: '{context}'\")\n",
    "        # Suggest replacement\n",
    "        if contraction in [\"d el\", \"del\"]:\n",
    "            print(\"    Suggested rule: Normalize 'd el' → 'del'\")\n",
    "        elif contraction in [\"a el\", \"al\"]:\n",
    "            print(\"    Suggested rule: Normalize 'a el' → 'al'\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  No Spanish contractions found.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns to Preserve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PATTERNS TO PRESERVE ANALYSIS \")\n",
    "print(\"Identifying patterns that should be preserved during cleaning\\n\")\n",
    "\n",
    "preservation_patterns = {  # Define patterns to check\n",
    "    \"dates\": r\"\\d{1,2}[\\/-]\\d{1,2}[\\/-]\\d{2,4}\",\n",
    "    \"times\": r\"\\d{1,2}:\\d{1,2}\",\n",
    "    \"measurements\": r\"\\d+[\\.\\,]?\\d*\\s*(?:mg|kg|g|ml|l|cm|mm|mmHg)\",\n",
    "    \"temperatures\": r\"\\d+[\\.\\,]\\d*\\s*°[CF]\",\n",
    "    \"decimal_numbers\": r\"\\d+[\\.\\,]\\d+\",\n",
    "    \"hyphenated_terms\": r\"[a-zA-ZáéíóúüñçÁÉÍÓÚÜÑÇàèìòù]+-[a-zA-ZáéíóúüñçÁÉÍÓÚÜÑÇàèìòù]+\",\n",
    "}\n",
    "\n",
    "# Analyze all sentences\n",
    "all_sentences = sentences[\"es\"] + sentences[\"ca\"]\n",
    "\n",
    "for pattern_name, pattern in preservation_patterns.items():\n",
    "    examples = []\n",
    "    count = 0\n",
    "\n",
    "    for sent_data in all_sentences:\n",
    "        text = sent_data[\"text\"]\n",
    "        for match in re.finditer(pattern, text):\n",
    "            count += 1\n",
    "            if len(examples) < 3:\n",
    "                matched_text = match.group(0)\n",
    "                start = max(0, match.start() - 20)\n",
    "                end = min(len(text), match.end() + 20)\n",
    "                context = text[start:end]\n",
    "                # This simulates applying the current cleaning function without actually calling it\n",
    "                cleaned_text = re.sub(r\"[^\\w\\s-]\", \"\", matched_text)\n",
    "                examples.append((matched_text, cleaned_text, context))\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{pattern_name.upper()} ({count} occurrences):\")\n",
    "    if examples:\n",
    "        print(f\"  Pattern: {pattern}\")\n",
    "        for i, (original, cleaned, context) in enumerate(examples):\n",
    "            print(f\"  Example {i+1}: '{original}' | Context: '{context}'\")\n",
    "            if original != cleaned:\n",
    "                print(f\"    Current cleaning would change it to: '{cleaned}'\\n\")\n",
    "            else:\n",
    "                print(f\"    Current cleaning would preserve it as is\\n\")\n",
    "    else:\n",
    "        print(\"  No examples found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation Removal Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PUNCTUATION REMOVAL IMPACT \")\n",
    "print(\"Analyzing the impact of punctuation removal in the current function\\n\")\n",
    "\n",
    "# Current function removes all punctuation except hyphens\n",
    "punct_pattern = r\"[^\\w\\s-]\"\n",
    "\n",
    "# Find all punctuation in the text corpus\n",
    "punct_counts = Counter()\n",
    "punct_contexts = {}\n",
    "\n",
    "# Categorize punctuation\n",
    "punct_categories = {\n",
    "    \"decimal_separators\": [\".\", \",\"],\n",
    "    \"date_separators\": [\"/\", \"-\"],\n",
    "    \"time_separators\": [\":\"],\n",
    "    \"unit_markers\": [\"°\", \"%\"],\n",
    "    \"apostrophes\": [\"'\"],\n",
    "    \"standard_punctuation\": [\".\", \",\", \";\", \":\", \"!\", \"?\"],\n",
    "    \"brackets\": [\"(\", \")\", \"[\", \"]\"],\n",
    "}\n",
    "\n",
    "all_sentences = sentences[\"es\"] + sentences[\"ca\"]\n",
    "\n",
    "for sent_data in all_sentences:\n",
    "    text = sent_data[\"text\"]\n",
    "    # Count all punctuation\n",
    "    for char in re.findall(punct_pattern, text):\n",
    "        punct_counts[char] += 1\n",
    "        if char not in punct_contexts:\n",
    "            pos = text.find(char)  # Find the position of this character\n",
    "            if pos >= 0:\n",
    "                start = max(0, pos - 15)\n",
    "                end = min(len(text), pos + 15)\n",
    "                punct_contexts[char] = text[start:end]\n",
    "\n",
    "print(\"PUNCTUATION USAGE BY CATEGORY:\")\n",
    "for category, chars in punct_categories.items():\n",
    "    category_chars = [c for c in chars if c in punct_counts]\n",
    "    if category_chars:\n",
    "        print(f\"\\n{category}:\")\n",
    "        for char in category_chars:\n",
    "            context = punct_contexts.get(char, \"\")\n",
    "            print(f\"  '{char}' ({punct_counts[char]} occurrences)\")\n",
    "            print(f\"    Context: '{context}'\")\n",
    "\n",
    "print(\"\\nCRITICAL INFORMATION LOSS FROM PUNCTUATION REMOVAL:\")\n",
    "critical_patterns = [(r\"\\d[\\.\\,]\\d\", \"Decimal numbers\"), (r\"\\d[/\\-]\\d\", \"Dates\"), (r\"\\d:\\d\", \"Times\"), (r\"\\d\\s*°[CF]\", \"Temperatures\")]\n",
    "\n",
    "for pattern, description in critical_patterns:\n",
    "    print(f\"\\n{description}:\")\n",
    "    examples_found = False\n",
    "\n",
    "    for sent_data in all_sentences:\n",
    "        text = sent_data[\"text\"]\n",
    "        for match in re.finditer(pattern, text):\n",
    "            examples_found = True\n",
    "            original = match.group(0)\n",
    "            cleaned = re.sub(punct_pattern, \"\", original)\n",
    "            start = max(0, match.start() - 10)\n",
    "            end = min(len(text), match.end() + 10)\n",
    "            context = text[start:end]\n",
    "\n",
    "            print(f\"  Original: '{original}'\")\n",
    "            print(f\"  Current cleaning result: '{cleaned}'\")\n",
    "            print(f\"  Context: '{context}'\\n\")\n",
    "            break  # Just one example per pattern\n",
    "\n",
    "    if not examples_found:\n",
    "        print(\"  No examples found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
