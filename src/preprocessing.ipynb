{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30fccf35",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7485d",
   "metadata": {},
   "source": [
    "The notebook handles the preprocessing of raw data for HMM models in multilingual medical text anonymized\n",
    "\n",
    "- Loading and parsing raw JSON\n",
    "- Language detection and cleaning\n",
    "- Tokenization and feature extraction\n",
    "- Creating observation sequences for HMM models\n",
    "- Save results for posterior training and evaluatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26aac27",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Set, Any, Optional\n",
    "import spacy\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "PATH_ROOT = os.path.dirname(os.getcwd())  # Path to the root of the project\n",
    "os.makedirs(os.path.join(PATH_ROOT, \"data\", \"processed\"), exist_ok=True)  # Directory output\n",
    "\n",
    "try:  # Load spaCy models for Spanish and Catalan\n",
    "    nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "    nlp_ca = spacy.load(\"ca_core_news_sm\")\n",
    "    print(\"spaCy models loaded successfully\\n\")\n",
    "except OSError:\n",
    "    print(\"ERROR: You need to download spaCy models\")\n",
    "    print(\"python -m spacy download es_core_news_sm\")\n",
    "    print(\"python -m spacy download ca_core_news_sm\")\n",
    "\n",
    "# Add project root to system path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "try:  # Import utility functions\n",
    "    from utils.json import *\n",
    "    from utils.language import *\n",
    "except ImportError:\n",
    "    print(\"Warning: Unable to import utility modules. Some functions may not work.\")\n",
    "\n",
    "# Load raw data\n",
    "FILE_TRAIN = os.path.join(PATH_ROOT, \"data\", \"raw\", \"train.json\")\n",
    "FILE_TEST = os.path.join(PATH_ROOT, \"data\", \"raw\", \"test.json\")\n",
    "\n",
    "# Baseline files\n",
    "FILE_TRAIN_BASELINE_PKL = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_baseline.pkl\")\n",
    "FILE_TEST_BASELINE_PKL = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_baseline.pkl\")\n",
    "\n",
    "# POS files\n",
    "FILE_TRAIN_POS_PKL = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_pos.pkl\")\n",
    "FILE_TEST_POS_PKL = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_pos.pkl\")\n",
    "\n",
    "# BIO POS files\n",
    "FILE_TRAIN_BIO_POS_PKL = os.path.join(PATH_ROOT, \"data\", \"processed\", \"train_bio_pos.pkl\")\n",
    "FILE_TEST_BIO_POS_PKL = os.path.join(PATH_ROOT, \"data\", \"processed\", \"test_bio_pos.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c8cf6",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_by_language(text: str, language: str, es_replacements: Dict[str, str] = None, ca_replacements: Dict[str, str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies language-specific cleaning to text while preserving critical patterns\n",
    "\n",
    "    Args:\n",
    "        text (str): The original text to clean\n",
    "        language (str): Language code ('es' or 'ca')\n",
    "        es_replacements: Optional custom Spanish replacements dictionary\n",
    "        ca_replacements: Optional custom Catalan replacements dictionary\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with important patterns preserved\n",
    "    \"\"\"\n",
    "    original_lower = text.lower()\n",
    "    temp_cleaned = original_lower\n",
    "\n",
    "    # Default Spanish replacements based on analysis\n",
    "    default_es_replacements = {\n",
    "        # Common medical abbreviations from the analysis\n",
    "        \"d.\": \"de\",\n",
    "        \"dr.\": \"doctor\",\n",
    "        \"dra.\": \"doctora\",\n",
    "        \"sr.\": \"señor\",\n",
    "        \"sra.\": \"señora\",\n",
    "        \"t.a.\": \"tensión arterial\",\n",
    "        \"hosp.\": \"hospital\",\n",
    "        \"tto.\": \"tratamiento\",\n",
    "        \"u.\": \"unidad\",\n",
    "        \"h.\": \"hospital\",\n",
    "        \"e.\": \"enfermedad\",\n",
    "        \"c.\": \"centro\",\n",
    "        \"n.\": \"número\",\n",
    "        \"t.\": \"tiempo\",\n",
    "        \"p.\": \"paciente\",\n",
    "        \"b.\": \"bacteria\",\n",
    "        # Contractions\n",
    "        \"a el\": \"al\",\n",
    "        \"d el\": \"del\",\n",
    "    }\n",
    "\n",
    "    # Default Catalan replacements based on analysis\n",
    "    default_ca_replacements = {\n",
    "        # Common medical abbreviations\n",
    "        \"dr.\": \"doctor\",\n",
    "        \"dra.\": \"doctora\",\n",
    "        \"sr.\": \"senyor\",\n",
    "        \"sra.\": \"senyora\",\n",
    "        \"h.\": \"hospital\",\n",
    "        \"t.\": \"temps\",\n",
    "        \"c.\": \"centre\",\n",
    "        \"u.\": \"unitat\",\n",
    "        \"i.\": \"informe\",\n",
    "        \"dia.\": \"dia\",\n",
    "    }\n",
    "\n",
    "    # Use provided dictionaries or fallback to defaults\n",
    "    es_replacements = es_replacements or default_es_replacements\n",
    "    ca_replacements = ca_replacements or default_ca_replacements\n",
    "\n",
    "    if language == \"es\":\n",
    "        # Apply all Spanish replacements\n",
    "        for orig, repl in es_replacements.items():\n",
    "            temp_cleaned = re.sub(r\"\\b\" + re.escape(orig) + r\"\\b\", repl, temp_cleaned)\n",
    "    elif language == \"ca\":\n",
    "        # Handle Catalan apostrophes\n",
    "        apostrophe_replacements = {\"d'\": \"de \", \"l'\": \"el \", \"s'\": \"se \", \"n'\": \"en \", \"m'\": \"me \", \"t'\": \"te \"}\n",
    "\n",
    "        for orig, repl in apostrophe_replacements.items():\n",
    "            temp_cleaned = temp_cleaned.replace(orig, repl)\n",
    "\n",
    "        # Apply all Catalan replacements\n",
    "        for orig, repl in ca_replacements.items():\n",
    "            if orig not in apostrophe_replacements:\n",
    "                temp_cleaned = re.sub(r\"\\b\" + re.escape(orig) + r\"\\b\", repl, temp_cleaned)\n",
    "\n",
    "    preserved_text = temp_cleaned  # Create a pattern registry to preserve important information\n",
    "    replacements = {}\n",
    "    placeholder_counter = 0\n",
    "\n",
    "    patterns = [  # Patterns to preserve\n",
    "        # Decimal numbers (both . and , as decimal separators)\n",
    "        (r\"\\b\\d+[\\.,]\\d+\\b\", \"DECIMAL_\"),\n",
    "        # Dates in various formats\n",
    "        (r\"\\b\\d{1,2}[\\/-]\\d{1,2}[\\/-]\\d{2,4}\\b\", \"DATE_\"),\n",
    "        (r\"\\b\\d{1,2}[\\.-]\\d{1,2}[\\.-]\\d{2,4}\\b\", \"DATE_\"),\n",
    "        # Times\n",
    "        (r\"\\b\\d{1,2}:\\d{1,2}(?::\\d{1,2})?\\b\", \"TIME_\"),\n",
    "        # Measurements with units (including temperature)\n",
    "        (r\"\\b\\d+(?:[\\.,]\\d+)?[\\s-]*(?:mg|kg|g|ml|l|cm|mm|mmHg)\\b\", \"MEASURE_\"),\n",
    "        (r\"\\b\\d+(?:[\\.,]\\d+)?[\\s-]*°[CF]\\b\", \"TEMP_\"),\n",
    "        (r\"\\b\\d+(?:[\\.,]\\d+)?[\\s-]*ºC\\b\", \"TEMP_\"),\n",
    "        # Percentages\n",
    "        (r\"\\b\\d+(?:[\\.,]\\d+)?[\\s-]*%\\b\", \"PERCENT_\"),\n",
    "        # Range expressions (common in dosing)\n",
    "        (r\"\\b\\d+-\\d+\\b\", \"RANGE_\"),\n",
    "        # Blood pressure values\n",
    "        (r\"\\b\\d+\\/\\d+\\b\", \"BP_\"),\n",
    "        # Lab values with units\n",
    "        (r\"\\b\\d+(?:[\\.,]\\d+)?[\\s-]*(?:g\\/dl|mg\\/dl|mmol\\/l|µg|ng\\/ml|ui\\/l|u\\/ml)\\b\", \"LAB_\"),\n",
    "    ]\n",
    "\n",
    "    # Replace each pattern with a placeholder\n",
    "    for pattern, prefix in patterns:\n",
    "        matches = re.finditer(pattern, preserved_text) # Find all matches for the pattern\n",
    "        for match in matches:\n",
    "            placeholder = f\"{prefix}{placeholder_counter}\" # Create a unique placeholder\n",
    "            original = match.group(0) # Get the original text\n",
    "            preserved_text = preserved_text.replace(original, placeholder, 1) # Replace the original text with the placeholder\n",
    "            replacements[placeholder] = original\n",
    "            placeholder_counter += 1\n",
    "\n",
    "    cleaned = re.sub(r\"[^\\w\\s-]\", \"\", preserved_text).strip()  # Apply general punctuation removal but keep hyphens\n",
    "\n",
    "    for placeholder, original in replacements.items():  # Restore the preserved patterns\n",
    "        cleaned = cleaned.replace(placeholder, original)\n",
    "\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned)  # Remove extra whitespace\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_label(token_span: Dict, annotations: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Determines the label for a token span based on character-level annotations\n",
    "\n",
    "    Parameters:\n",
    "        token_span (Dict): Token info with 'start' and 'end' character positions\n",
    "        annotations (List[Dict]): List of annotation spans from the raw data\n",
    "\n",
    "    Returns:\n",
    "        str: Label\n",
    "    \"\"\"\n",
    "    token_start = token_span[\"start\"]\n",
    "    token_end = token_span[\"end\"]\n",
    "\n",
    "    overlaps = [] # Store overlapping annotations with priority\n",
    "    priority = {\"UNC\": 4, \"USCO\": 3, \"NEG\": 2, \"NSCO\": 1}\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if \"value\" not in annotation or \"labels\" not in annotation[\"value\"] or not annotation[\"value\"][\"labels\"]:\n",
    "            continue\n",
    "\n",
    "        anno_start = annotation[\"value\"][\"start\"]\n",
    "        anno_end = annotation[\"value\"][\"end\"]\n",
    "        label = annotation[\"value\"][\"labels\"][0]\n",
    "\n",
    "        if label not in priority:  # Ignore irrelevant labels if any\n",
    "            continue\n",
    "        # Calculate overlap fraction relative to the token length\n",
    "        overlap_start = max(token_start, anno_start) \n",
    "        overlap_end = min(token_end, anno_end)\n",
    "        overlap_length = max(0, overlap_end - overlap_start)\n",
    "        token_length = max(1, token_end - token_start)  # Avoid division by zero\n",
    "\n",
    "        if overlap_length / token_length > 0.5: # Define overlap threshold (e.g., > 50% of token must overlap)\n",
    "            overlaps.append({\"label\": label, \"start\": anno_start, \"end\": anno_end, \"priority\": priority[label]})\n",
    "\n",
    "    if not overlaps:\n",
    "        return \"O\" # No overlapping annotations\n",
    "\n",
    "    best_overlap = max(overlaps, key=lambda x: x[\"priority\"]) # Find the highest priority overlapping annotation\n",
    "    best_label = best_overlap[\"label\"]\n",
    "\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2a2fe",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4aea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_hmm(data: List[Dict], include_pos: bool = False, use_bio: bool = False, use_context_window: bool = False, window_size: int = 1) -> Dict:\n",
    "    \"\"\"\n",
    "    Processes raw data for HMM training with sentence-level language detection\n",
    "\n",
    "    Parameters:\n",
    "        data (List[Dict]): List of documents with annotations\n",
    "        include_pos (bool): Whether to include POS tags in observations\n",
    "        use_bio (bool): Whether to use BIO tagging scheme for states\n",
    "        use_context_window (bool): Whether to include context window features\n",
    "        window_size (int): Size of context window (each side) if used\n",
    "\n",
    "    Returns:\n",
    "        Dict: Processed data with sequences, vocabulary, state space, etc\n",
    "    \"\"\"\n",
    "\n",
    "    sequences = []  # List to store detailed info per sequence\n",
    "    observations = []  # List of observation sequences for HMM\n",
    "    states = []  # List of state sequences for HMM\n",
    "    \n",
    "    vocab = set()  # Set of unique observations\n",
    "    pos_tags = set()  # Set of unique POS tags encountered\n",
    "    doc_languages = set()  # Track languages found at document level\n",
    "\n",
    "    if use_bio: # Define state space based on whether BIO tagging is used\n",
    "        state_space = {\"O\", \"B-NEG\", \"I-NEG\", \"B-NSCO\", \"I-NSCO\", \"B-UNC\", \"I-UNC\", \"B-USCO\", \"I-USCO\"}\n",
    "    else:\n",
    "        state_space = {\"O\", \"NEG\", \"NSCO\", \"UNC\", \"USCO\"}\n",
    "\n",
    "    print(f\"Starting data preparation with include_pos={include_pos}, use_bio={use_bio}, use_context_window={use_context_window}...\")\n",
    "    processed_docs = 0\n",
    "\n",
    "    for i, document in enumerate(data):\n",
    "        if \"data\" not in document or \"text\" not in document[\"data\"]:\n",
    "            print(f\"Warning: Skipping document {i} due to missing 'data' or 'text'.\")\n",
    "            continue\n",
    "\n",
    "        text = document[\"data\"][\"text\"]\n",
    "        if not text or text.isspace():\n",
    "            print(f\"Warning: Skipping document {i} due to empty text.\")\n",
    "            continue\n",
    "\n",
    "        annotations = []  # Extract annotations for this document\n",
    "        if \"predictions\" in document and document[\"predictions\"]:\n",
    "            for prediction in document[\"predictions\"]:\n",
    "                if \"result\" in prediction:\n",
    "                    annotations.extend(prediction[\"result\"])\n",
    "\n",
    "        # Process the document\n",
    "        doc_observations = []\n",
    "        doc_states = []\n",
    "        doc_tokens = []  # Detailed token info\n",
    "\n",
    "        try:  # Segment the document into sentences (using default language model initially)\n",
    "            initial_doc = nlp_es(text)  # Use Spanish model for initial segmentation\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Initial document segmentation failed for document {i}. Error: {e}\")\n",
    "            print(f\"Text snippet: {text[:200]}\")\n",
    "            continue\n",
    "\n",
    "        for sent in initial_doc.sents:  # Process each sentence\n",
    "            sent_text = sent.text\n",
    "            sent_start = sent.start_char\n",
    "\n",
    "            if not sent_text or sent_text.isspace():  # Skip empty sentences\n",
    "                continue\n",
    "\n",
    "            sent_lang = detect_sentence_language(sent_text) # Detect language for THIS SPECIFIC SENTENCE\n",
    "            doc_languages.add(sent_lang)\n",
    "\n",
    "            nlp = nlp_ca if sent_lang == \"ca\" else nlp_es  # Select appropriate language model for this sentence\n",
    "\n",
    "            try:  # Process the sentence with the correct language model\n",
    "                spacy_sent = nlp(sent_text)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Sentence processing failed. Language: {sent_lang}. Error: {e}\")\n",
    "                print(f\"Sentence: {sent_text}\")\n",
    "                continue\n",
    "\n",
    "            # Process each token in the sentence\n",
    "            for token in spacy_sent:\n",
    "                if token.is_space:\n",
    "                    continue\n",
    "\n",
    "                # Calculate absolute position in the document\n",
    "                abs_token_start = sent_start + token.idx\n",
    "                abs_token_end = abs_token_start + len(token.text)\n",
    "\n",
    "                token_span = {\"text\": token.text, \"start\": abs_token_start, \"end\": abs_token_end}  # Create token span for annotation lookup\n",
    "\n",
    "                label = get_token_label(token_span, annotations)  # TODO Bio Here in future\n",
    "\n",
    "                cleaned_text = clean_text_by_language(token.text, sent_lang)  # Clean token text using the sentence language\n",
    "\n",
    "                if not cleaned_text:  # Skip if cleaning results in empty string\n",
    "                    continue\n",
    "\n",
    "                token_info = {  # Token details\n",
    "                    \"text\": token.text,\n",
    "                    \"cleaned_text\": cleaned_text.lower(),\n",
    "                    \"pos\": token.pos_,\n",
    "                    \"lemma\": token.lemma_,\n",
    "                    \"start\": abs_token_start,\n",
    "                    \"end\": abs_token_end,\n",
    "                    \"label\": label,\n",
    "                    \"language\": sent_lang,\n",
    "                }\n",
    "\n",
    "                doc_tokens.append(token_info)\n",
    "\n",
    "        for i, token_info in enumerate(doc_tokens):  # Create observation based on features\n",
    "            observation = token_info[\"cleaned_text\"] # Simple word only # TODO\n",
    "\n",
    "            doc_observations.append(observation) # Add to sequences\n",
    "            doc_states.append(token_info[\"label\"])\n",
    "\n",
    "            vocab.add(observation) # Add to vocabulary\n",
    "\n",
    "        if doc_observations: # Add document to sequences if not empty\n",
    "            observations.append(doc_observations)\n",
    "            states.append(doc_states)\n",
    "            sequences.append({\"observations\": doc_observations, \"states\": doc_states, \"tokens\": doc_tokens})\n",
    "\n",
    "        processed_docs += 1\n",
    "        if processed_docs % 100 == 0:\n",
    "            print(f\"  Processed {processed_docs}/{len(data)} documents...\")\n",
    "\n",
    "    print(f\"Finished data preparation. Processed {processed_docs} documents.\")\n",
    "    print(f\"Found {len(vocab)} unique observations in vocabulary.\") \n",
    "    print(f\"Document languages encountered: {doc_languages}\")\n",
    "\n",
    "    return {\n",
    "        \"sequences\": sequences,\n",
    "        \"observations\": observations,\n",
    "        \"states\": states,\n",
    "        \"vocabulary\": vocab,\n",
    "        \"state_space\": state_space,\n",
    "        \"doc_languages\": doc_languages,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_processed_data(data: Dict, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves processed data to a pickle file\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Processed data saved successfully to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save processed data to {output_file}. Error: {e}\")\n",
    "\n",
    "\n",
    "def load_processed_data(input_file: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Loads processed data from a pickle file\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(input_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Processed data loaded successfully from {input_file}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Processed data file not found at {input_file}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e1634",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading raw data...\")\n",
    "train_data = load_json_data(FILE_TRAIN)\n",
    "test_data = load_json_data(FILE_TEST)\n",
    "print(f\"Loaded {len(train_data)} training documents\")\n",
    "print(f\"Loaded {len(test_data)} test documents\")\n",
    "\n",
    "\n",
    "# Process Baseline Data (Without POS or BIO)\n",
    "print(\"\\nProcessing data for baseline model...\")\n",
    "train_baseline = prepare_data_for_hmm(train_data, include_pos=False, use_bio=False)\n",
    "test_baseline = prepare_data_for_hmm(test_data, include_pos=False, use_bio=False)\n",
    "\n",
    "# Save processed data\n",
    "save_processed_data(train_baseline, FILE_TRAIN_BASELINE_PKL)\n",
    "save_processed_data(test_baseline, FILE_TEST_BASELINE_PKL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
